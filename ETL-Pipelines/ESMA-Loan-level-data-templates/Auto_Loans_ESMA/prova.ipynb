{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aut_etl_pipeline.utils.bronze_profile_funcs import get_csv_files, profile_data\n",
    "from src.aut_etl_pipeline.utils.validation_rules import asset_schema\n",
    "from cerberus import Validator\n",
    "\n",
    "validator = Validator(asset_schema())\n",
    "files = get_csv_files(RAW_BUCKET, \"dl_data/downloaded-data/AUT/your_dl_code\", \"Loan_Data\", \"assets\")\n",
    "for f in files:\n",
    "    clean, dirty = profile_data(RAW_BUCKET, f, \"assets\", validator)\n",
    "    print(f\"Clean: {len(clean)}, Dirty: {len(dirty)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guida Step-by-Step: Test della Pipeline ESMA Auto Loans in un Notebook\n",
    "\n",
    "Questa guida ti permette di testare i **moduli Spark** della pipeline ESMA Auto Loans direttamente da un notebook (Colab, Dataproc, Jupyter).  \n",
    "L’obiettivo è simulare una run di pipeline su dati reali leggendo e scrivendo da GCS, senza passare da Airflow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Prerequisiti**\n",
    "\n",
    "- Python ≥ 3.8\n",
    "- Librerie: `pyspark`, `google-cloud-storage`, `pandas`, `cerberus`, `delta-spark`\n",
    "- Service account GCP con permessi su bucket GCS\n",
    "- File di input caricati su GCS (ad esempio, CSV in `dl_data/downloaded-data/AUT/<dl_code>/`)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Setup Ambiente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installazione delle dipendenze (Colab/Dataproc)\n",
    "!pip install pyspark==3.3.1 delta-spark==2.1.0 google-cloud-storage cerberus pandas\n",
    "\n",
    "import os\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppure:**  \n",
    "Assicurati che la tua variabile di ambiente GOOGLE_APPLICATION_CREDENTIALS punti al service account json!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/to/your/service-account.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Setup SparkSession con Delta e GCS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = SparkSession.builder.appName(\"esma_test\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.1.0\") \\\n",
    "    .config(\"spark.delta.logStore.gs.impl\", \"io.delta.storage.GCSLogStore\") \\\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\") \\\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Impostazione Parametri di Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sostituisci con i tuoi valori reali di test\n",
    "PROJECT_ID = \"your_project_id\"\n",
    "RAW_BUCKET = \"your_raw_bucket\"\n",
    "DATA_BUCKET = \"your_data_bucket\"\n",
    "DL_CODE = \"test_deal_code\"  # esempio: 'AUT1234'\n",
    "INGESTION_DATE = \"2025-05-31\"\n",
    "SOURCE_PREFIX = f\"dl_data/downloaded-data/AUT/{DL_CODE}\"\n",
    "TARGET_BRONZE_PREFIX = \"AUT/bronze/assets\"\n",
    "TARGET_SILVER_PREFIX = \"AUT/silver/assets\"\n",
    "FILE_KEY = \"Loan_Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Test Profilazione Bronze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aut_etl_pipeline.profile_bronze_data import profile_bronze_data\n",
    "\n",
    "# Lancia la profilazione: genera i file clean_dump/ e dirty_dump/ su GCS\n",
    "profile_bronze_data(\n",
    "    raw_bucketname=RAW_BUCKET,\n",
    "    data_bucketname=DATA_BUCKET,\n",
    "    source_prefix=SOURCE_PREFIX,\n",
    "    file_key=FILE_KEY,\n",
    "    data_type=\"assets\",\n",
    "    ingestion_date=INGESTION_DATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Controlla su GCS:**  \n",
    "Dovresti trovare i file CSV in `gs://<DATA_BUCKET>/clean_dump/assets/` e `dirty_dump/assets/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Test Bronze Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aut_etl_pipeline.generate_bronze_tables import generate_bronze_tables\n",
    "\n",
    "generate_bronze_tables(\n",
    "    spark=spark,\n",
    "    data_bucketname=DATA_BUCKET,\n",
    "    source_prefix=SOURCE_PREFIX,\n",
    "    target_prefix=TARGET_BRONZE_PREFIX,\n",
    "    data_type=\"assets\",\n",
    "    ingestion_date=INGESTION_DATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Controlla su GCS:**  \n",
    "Dovresti trovare le tabelle Delta in `gs://<DATA_BUCKET>/AUT/bronze/assets/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Test Silver Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aut_etl_pipeline.generate_asset_silver import generate_asset_silver\n",
    "\n",
    "generate_asset_silver(\n",
    "    spark=spark,\n",
    "    bucket_name=DATA_BUCKET,\n",
    "    source_prefix=TARGET_BRONZE_PREFIX,\n",
    "    target_prefix=TARGET_SILVER_PREFIX,\n",
    "    dl_code=DL_CODE,\n",
    "    ingestion_date=INGESTION_DATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Controlla su GCS:**  \n",
    "Dovresti trovare i file Parquet in `gs://<DATA_BUCKET>/AUT/silver/assets/lease_info_table/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Lettura e Verifica Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggi la tabella silver per verificare il risultato\n",
    "df = spark.read.parquet(f\"gs://{DATA_BUCKET}/AUT/silver/assets/lease_info_table/\")\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. (Opzionale) Test Deal Details**\n",
    "\n",
    "Applica lo stesso schema, cambiando `data_type`, prefissi e funzioni (usa `generate_deal_details_bronze` e `generate_deal_details_silver`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Debug**\n",
    "\n",
    "- Se ricevi errori, stampa i log, controlla i permessi GCS e la presenza dei file di input.\n",
    "- Se hai errori di dipendenze, assicurati che tutte le versioni siano compatibili.\n",
    "- Puoi modificare i parametri o lavorare su un solo DL_CODE per debug più veloce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **11. Pulizia (Cleanup)**\n",
    "\n",
    "Se vuoi rimuovere i file di test dai bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "bucket = client.get_bucket(DATA_BUCKET)\n",
    "for blob in bucket.list_blobs(prefix=\"AUT/\"):\n",
    "    print(\"Deleting\", blob.name)\n",
    "    blob.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Simualtion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime righe del dataframe letto:\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|FIELD CODE|          FIELD NAME|   CONTENT TO REPORT|        LOAN 1|        LOAN 2|        LOAN 3|        LOAN 4|        LOAN 5|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|     AUTL1|   Unique Identifier|The unique identi...|AUT-2024-001-A|AUT-2024-002-B|AUT-2024-003-C|AUT-2024-004-D|AUT-2024-005-E|\n",
      "|     AUTL2|Original Underlyi...|Unique underlying...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL3|New Underlying Ex...|New identifier if...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL4|Original Obligor ...|Original unique o...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "|     AUTL5|New Obligor Ident...|New identifier if...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- FIELD CODE: string (nullable = true)\n",
      " |-- FIELD NAME: string (nullable = true)\n",
      " |-- CONTENT TO REPORT: string (nullable = true)\n",
      " |-- LOAN 1: string (nullable = true)\n",
      " |-- LOAN 2: string (nullable = true)\n",
      " |-- LOAN 3: string (nullable = true)\n",
      " |-- LOAN 4: string (nullable = true)\n",
      " |-- LOAN 5: string (nullable = true)\n",
      "\n",
      "Numero totale record: 84\n",
      "Conteggio valori nulli per colonna:\n",
      "FIELD CODE: 0\n",
      "FIELD NAME: 0\n",
      "CONTENT TO REPORT: 0\n",
      "LOAN 1: 14\n",
      "LOAN 2: 14\n",
      "LOAN 3: 12\n",
      "LOAN 4: 13\n",
      "LOAN 5: 14\n",
      "Dati scritti in formato parquet su ./output/bronze_table\n",
      "Prime righe dal parquet scritto:\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|FIELD CODE|          FIELD NAME|   CONTENT TO REPORT|        LOAN 1|        LOAN 2|        LOAN 3|        LOAN 4|        LOAN 5|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|     AUTL1|   Unique Identifier|The unique identi...|AUT-2024-001-A|AUT-2024-002-B|AUT-2024-003-C|AUT-2024-004-D|AUT-2024-005-E|\n",
      "|     AUTL2|Original Underlyi...|Unique underlying...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL3|New Underlying Ex...|New identifier if...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL4|Original Obligor ...|Original unique o...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "|     AUTL5|New Obligor Ident...|New identifier if...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# ---- 1. Setup ambiente Spark ----\n",
    "spark = SparkSession.builder.appName(\"esma_local_test\").getOrCreate()\n",
    "\n",
    "# ---- 2. Leggi file Excel o CSV locale ----\n",
    "\n",
    "# Esempio con CSV\n",
    "csv_path = \"/Users/hp2/Deeploans/deeploans/ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/Auto Loans Synthetic Dataset.csv\"\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "\n",
    "# Esempio con Excel (richiede pandas + openpyxl)\n",
    "# excel_path = \"./dati/Loan_Data.xlsx\"\n",
    "# pdf = pd.read_excel(excel_path)\n",
    "# df = spark.createDataFrame(pdf)\n",
    "\n",
    "print(\"Prime righe del dataframe letto:\")\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "\n",
    "# ---- 3. Profilazione semplice dei dati ----\n",
    "\n",
    "print(\"Numero totale record:\", df.count())\n",
    "print(\"Conteggio valori nulli per colonna:\")\n",
    "for col in df.columns:\n",
    "    null_count = df.filter(df[col].isNull()).count()\n",
    "    print(f\"{col}: {null_count}\")\n",
    "\n",
    "# ---- 4. (Facoltativo) Scrivi i dati risultanti su disco ----\n",
    "\n",
    "output_path = \"./output/bronze_table\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Dati scritti in formato parquet su {output_path}\")\n",
    "\n",
    "# ---- 5. Lettura del parquet per verifica ----\n",
    "\n",
    "df2 = spark.read.parquet(output_path)\n",
    "print(\"Prime righe dal parquet scritto:\")\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime righe del dataframe letto:\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|FIELD CODE|          FIELD NAME|   CONTENT TO REPORT|        LOAN 1|        LOAN 2|        LOAN 3|        LOAN 4|        LOAN 5|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|     AUTL1|   Unique Identifier|The unique identi...|AUT-2024-001-A|AUT-2024-002-B|AUT-2024-003-C|AUT-2024-004-D|AUT-2024-005-E|\n",
      "|     AUTL2|Original Underlyi...|Unique underlying...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL3|New Underlying Ex...|New identifier if...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL4|Original Obligor ...|Original unique o...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "|     AUTL5|New Obligor Ident...|New identifier if...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- FIELD CODE: string (nullable = true)\n",
      " |-- FIELD NAME: string (nullable = true)\n",
      " |-- CONTENT TO REPORT: string (nullable = true)\n",
      " |-- LOAN 1: string (nullable = true)\n",
      " |-- LOAN 2: string (nullable = true)\n",
      " |-- LOAN 3: string (nullable = true)\n",
      " |-- LOAN 4: string (nullable = true)\n",
      " |-- LOAN 5: string (nullable = true)\n",
      "\n",
      "Dopo pulizia righe totalmente nulle:\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|FIELD CODE|          FIELD NAME|   CONTENT TO REPORT|        LOAN 1|        LOAN 2|        LOAN 3|        LOAN 4|        LOAN 5|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|     AUTL1|   Unique Identifier|The unique identi...|AUT-2024-001-A|AUT-2024-002-B|AUT-2024-003-C|AUT-2024-004-D|AUT-2024-005-E|\n",
      "|     AUTL2|Original Underlyi...|Unique underlying...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL3|New Underlying Ex...|New identifier if...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL4|Original Obligor ...|Original unique o...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "|     AUTL5|New Obligor Ident...|New identifier if...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "Dati puliti scritti in ./output/bronze_table\n",
      "Verifica parquet:\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|FIELD CODE|          FIELD NAME|   CONTENT TO REPORT|        LOAN 1|        LOAN 2|        LOAN 3|        LOAN 4|        LOAN 5|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|     AUTL1|   Unique Identifier|The unique identi...|AUT-2024-001-A|AUT-2024-002-B|AUT-2024-003-C|AUT-2024-004-D|AUT-2024-005-E|\n",
      "|     AUTL2|Original Underlyi...|Unique underlying...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL3|New Underlying Ex...|New identifier if...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL4|Original Obligor ...|Original unique o...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "|     AUTL5|New Obligor Ident...|New identifier if...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# 1. Setup Spark locale\n",
    "spark = SparkSession.builder.appName(\"esma_local_test\").getOrCreate()\n",
    "\n",
    "# 2. Scegli il file da testare\n",
    "csv_path = \"/Users/hp2/Deeploans/deeploans/ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/Auto Loans Synthetic Dataset.csv\"         # Oppure...\n",
    "excel_path = \"/Users/hp2/Deeploans/deeploans/ETL-Pipelines/ESMA-Loan-level-data-templates/annex5_underlying_exposures-automobile.xlsx\"\n",
    "\n",
    "# 3. Leggi il file\n",
    "if os.path.exists(csv_path):\n",
    "    df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "elif os.path.exists(excel_path):\n",
    "    pdf = pd.read_excel(excel_path)\n",
    "    df = spark.createDataFrame(pdf)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Nessun file CSV o Excel trovato!\")\n",
    "\n",
    "print(\"Prime righe del dataframe letto:\")\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "\n",
    "# 4. Esegui una semplice pulizia: rimuovi righe con tutti valori nulli\n",
    "df_clean = df.na.drop(how=\"all\")\n",
    "print(\"Dopo pulizia righe totalmente nulle:\")\n",
    "df_clean.show(5)\n",
    "\n",
    "# 5. Scrivi su disco in formato parquet\n",
    "output_path = \"./output/bronze_table\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "df_clean.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Dati puliti scritti in {output_path}\")\n",
    "\n",
    "# 6. Leggi e mostra il parquet appena scritto\n",
    "df_verifica = spark.read.parquet(output_path)\n",
    "print(\"Verifica parquet:\")\n",
    "df_verifica.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime righe del dataframe letto:\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|FIELD CODE|          FIELD NAME|   CONTENT TO REPORT|        LOAN 1|        LOAN 2|        LOAN 3|        LOAN 4|        LOAN 5|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|     AUTL1|   Unique Identifier|The unique identi...|AUT-2024-001-A|AUT-2024-002-B|AUT-2024-003-C|AUT-2024-004-D|AUT-2024-005-E|\n",
      "|     AUTL2|Original Underlyi...|Unique underlying...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL3|New Underlying Ex...|New identifier if...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL4|Original Obligor ...|Original unique o...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "|     AUTL5|New Obligor Ident...|New identifier if...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- FIELD CODE: string (nullable = true)\n",
      " |-- FIELD NAME: string (nullable = true)\n",
      " |-- CONTENT TO REPORT: string (nullable = true)\n",
      " |-- LOAN 1: string (nullable = true)\n",
      " |-- LOAN 2: string (nullable = true)\n",
      " |-- LOAN 3: string (nullable = true)\n",
      " |-- LOAN 4: string (nullable = true)\n",
      " |-- LOAN 5: string (nullable = true)\n",
      "\n",
      "Colonne obbligatorie per la validazione: ['LOAN 1', 'LOAN 2', 'LOAN 3', 'LOAN 4', 'LOAN 5']\n",
      "Righe CLEAN:\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|FIELD CODE|          FIELD NAME|   CONTENT TO REPORT|        LOAN 1|        LOAN 2|        LOAN 3|        LOAN 4|        LOAN 5|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|     AUTL1|   Unique Identifier|The unique identi...|AUT-2024-001-A|AUT-2024-002-B|AUT-2024-003-C|AUT-2024-004-D|AUT-2024-005-E|\n",
      "|     AUTL2|Original Underlyi...|Unique underlying...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL3|New Underlying Ex...|New identifier if...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|\n",
      "|     AUTL4|Original Obligor ...|Original unique o...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "|     AUTL5|New Obligor Ident...|New identifier if...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "Righe DIRTY:\n",
      "+----------+--------------------+--------------------+------+------+------+------+------+\n",
      "|FIELD CODE|          FIELD NAME|   CONTENT TO REPORT|LOAN 1|LOAN 2|LOAN 3|LOAN 4|LOAN 5|\n",
      "+----------+--------------------+--------------------+------+------+------+------+------+\n",
      "|    AUTL73|        Default Date|     Date of default|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|    AUTL22|      Special Scheme|Special public se...|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|    AUTL71|Reason for Defaul...|Reason if in default|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|    AUTL33|Principal Grace P...|Principal grace p...|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|    AUTL38|      Balloon Amount|Principal repayme...|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "+----------+--------------------+--------------------+------+------+------+------+------+\n",
      "only showing top 5 rows\n",
      "Dump clean: ./output/clean_dump/assets\n",
      "Dump dirty: ./output/dirty_dump/assets\n",
      "Bronze table scritta in formato parquet su ./output/bronze_table\n",
      "Silver table scritta in formato parquet su ./output/silver_table\n",
      "Verifica parquet silver:\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+------------+\n",
      "|FIELD CODE|          FIELD NAME|   CONTENT TO REPORT|        LOAN 1|        LOAN 2|        LOAN 3|        LOAN 4|        LOAN 5|field_length|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+------------+\n",
      "|     AUTL1|   Unique Identifier|The unique identi...|AUT-2024-001-A|AUT-2024-002-B|AUT-2024-003-C|AUT-2024-004-D|AUT-2024-005-E|           5|\n",
      "|     AUTL2|Original Underlyi...|Unique underlying...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|           5|\n",
      "|     AUTL3|New Underlying Ex...|New identifier if...|  ORIG-EXP-001|  ORIG-EXP-002|  ORIG-EXP-003|  ORIG-EXP-004|  ORIG-EXP-005|           5|\n",
      "|     AUTL4|Original Obligor ...|Original unique o...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|           5|\n",
      "|     AUTL5|New Obligor Ident...|New identifier if...| OBLG-2024-001| OBLG-2024-002| OBLG-2024-003| OBLG-2024-004| OBLG-2024-005|           5|\n",
      "+----------+--------------------+--------------------+--------------+--------------+--------------+--------------+--------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# --------- 1. Setup Spark locale ---------\n",
    "spark = SparkSession.builder.appName(\"esma_local_test\").getOrCreate()\n",
    "\n",
    "# --------- 2. Scegli il file da testare ---------\n",
    "csv_path = \"/Users/hp2/Deeploans/deeploans/ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/Auto Loans Synthetic Dataset.csv\"         # Oppure...\n",
    "excel_path = \"/Users/hp2/Deeploans/deeploans/ETL-Pipelines/ESMA-Loan-level-data-templates/annex5_underlying_exposures-automobile.xlsx\"\n",
    "\n",
    "# --------- 3. Leggi il file ---------\n",
    "if os.path.exists(csv_path):\n",
    "    df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "elif os.path.exists(excel_path):\n",
    "    pdf = pd.read_excel(excel_path)\n",
    "    df = spark.createDataFrame(pdf)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Nessun file CSV o Excel trovato!\")\n",
    "\n",
    "print(\"Prime righe del dataframe letto:\")\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "\n",
    "# --------- 4. Simula clean/dirty dump ---------\n",
    "# (ESMA: una riga \"clean\" se tutte le colonne obbligatorie NON sono nulle)\n",
    "# Scegli alcune colonne obbligatorie (ad esempio modificare secondo schema reale)\n",
    "obbligatorie = [col for col in df.columns if col.lower().startswith(\"loan\") or col.lower().startswith(\"asset\")]\n",
    "\n",
    "if not obbligatorie:\n",
    "    # Se non ci sono colonne che soddisfano il filtro, prendi le prime due (solo per test!)\n",
    "    obbligatorie = df.columns[:2]\n",
    "\n",
    "print(f\"Colonne obbligatorie per la validazione: {obbligatorie}\")\n",
    "\n",
    "df_clean = df\n",
    "for col in obbligatorie:\n",
    "    df_clean = df_clean.filter(df_clean[col].isNotNull())\n",
    "\n",
    "df_dirty = df.subtract(df_clean)\n",
    "\n",
    "print(\"Righe CLEAN:\")\n",
    "df_clean.show(5)\n",
    "print(\"Righe DIRTY:\")\n",
    "df_dirty.show(5)\n",
    "\n",
    "# --------- 5. Scrivi clean/dirty dump come CSV ---------\n",
    "output_clean = \"./output/clean_dump/assets\"\n",
    "output_dirty = \"./output/dirty_dump/assets\"\n",
    "os.makedirs(output_clean, exist_ok=True)\n",
    "os.makedirs(output_dirty, exist_ok=True)\n",
    "df_clean.write.mode(\"overwrite\").csv(output_clean, header=True)\n",
    "df_dirty.write.mode(\"overwrite\").csv(output_dirty, header=True)\n",
    "print(f\"Dump clean: {output_clean}\\nDump dirty: {output_dirty}\")\n",
    "\n",
    "# --------- 6. Bronze: salva tutto come Parquet ---------\n",
    "bronze_path = \"./output/bronze_table\"\n",
    "os.makedirs(bronze_path, exist_ok=True)\n",
    "df_clean.write.mode(\"overwrite\").parquet(bronze_path)\n",
    "print(f\"Bronze table scritta in formato parquet su {bronze_path}\")\n",
    "\n",
    "# --------- 7. Silver: esempio semplice di arricchimento/trasformazione ---------\n",
    "# Per test: aggiungiamo una colonna calcolata, es: length di un campo stringa\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "if df_clean.columns:\n",
    "    col_da_arricchire = df_clean.columns[0]  # Scegli una colonna qualsiasi per esempio\n",
    "    df_silver = df_clean.withColumn(\"field_length\", length(df_clean[col_da_arricchire]))\n",
    "else:\n",
    "    df_silver = df_clean\n",
    "\n",
    "silver_path = \"./output/silver_table\"\n",
    "os.makedirs(silver_path, exist_ok=True)\n",
    "df_silver.write.mode(\"overwrite\").parquet(silver_path)\n",
    "print(f\"Silver table scritta in formato parquet su {silver_path}\")\n",
    "\n",
    "# --------- 8. Lettura finale di verifica ---------\n",
    "print(\"Verifica parquet silver:\")\n",
    "df_verifica = spark.read.parquet(silver_path)\n",
    "df_verifica.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:24:27,833 - src.aut_etl_pipeline.profile_bronze_data - INFO - Start ASSETS BRONZE PROFILING job.\n"
     ]
    },
    {
     "ename": "SchemaError",
     "evalue": "{'AUTL16': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL20': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL24': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL25': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL26': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_int']}]}], 'AUTL29': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL30': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL31': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL33': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL37': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL38': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL39': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL40': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL43': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL44': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_int']}]}], 'AUTL45': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL46': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL47': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_int']}]}], 'AUTL48': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL49': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL50': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL51': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL52': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL59': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL6': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL60': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL61': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL62': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL63': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL64': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL65': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL66': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL67': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL68': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL69': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_int']}]}], 'AUTL7': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL72': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL73': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL74': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL75': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL76': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL77': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL78': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL8': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL9': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}]}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSchemaError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maut_etl_pipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerate_asset_silver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_asset_silver\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 4. Adatta le funzioni per accettare path locali\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Esempio: modifica le funzioni per accettare un flag \"local_mode\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# oppure, se usano direttamente GCS, copia temporaneamente il codice \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Profilazione bronze\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mprofile_bronze_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_bucketname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_bucketname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./dati/Loan_Data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoan_Data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mingestion_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2025-06-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_output_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m generate_bronze_tables(\n\u001b[1;32m     40\u001b[0m     spark\u001b[38;5;241m=\u001b[39mspark,\n\u001b[1;32m     41\u001b[0m     data_bucketname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     local_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m generate_asset_silver(\n\u001b[1;32m     50\u001b[0m     spark\u001b[38;5;241m=\u001b[39mspark,\n\u001b[1;32m     51\u001b[0m     bucket_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     local_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     57\u001b[0m )\n",
      "File \u001b[0;32m~/Deeploans/deeploans/ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/src/aut_etl_pipeline/profile_bronze_data.py:49\u001b[0m, in \u001b[0;36mprofile_bronze_data\u001b[0;34m(raw_bucketname, data_bucketname, source_prefix, file_key, data_type, ingestion_date, local_mode, local_output_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m     custom_coerce \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: TO_DATE,\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: TO_NUMBER,\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_int\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x)\n\u001b[1;32m     46\u001b[0m }\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massets\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 49\u001b[0m         validator \u001b[38;5;241m=\u001b[39m \u001b[43mValidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masset_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoerce_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_coerce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo schema defined for data_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeploans/lib/python3.10/site-packages/cerberus/validator.py:183\u001b[0m, in \u001b[0;36mBareValidator.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" The error handler used to format :attr:`~cerberus.Validator.errors`\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    and process submitted errors with\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    :meth:`~cerberus.Validator._error`.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    Type: :class:`~cerberus.errors.BaseErrorHandler` \"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__store_config(args, kwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_unknown \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_unknown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_all \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequire_all\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeploans/lib/python3.10/site-packages/cerberus/validator.py:594\u001b[0m, in \u001b[0;36mBareValidator.schema\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;241m=\u001b[39m schema\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;241m=\u001b[39m \u001b[43mDefinitionSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeploans/lib/python3.10/site-packages/cerberus/schema.py:82\u001b[0m, in \u001b[0;36mDefinitionSchema.__init__\u001b[0;34m(self, validator, schema)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema_validator \u001b[38;5;241m=\u001b[39m SchemaValidator(\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m     allow_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_schema,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     target_validator\u001b[38;5;241m=\u001b[39mvalidator,\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     81\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpand(schema)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m schema\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeploans/lib/python3.10/site-packages/cerberus/schema.py:265\u001b[0m, in \u001b[0;36mDefinitionSchema.validate\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m    263\u001b[0m _hash \u001b[38;5;241m=\u001b[39m (mapping_hash(schema), mapping_hash(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator\u001b[38;5;241m.\u001b[39mtypes_mapping))\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _hash \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator\u001b[38;5;241m.\u001b[39m_valid_schemas:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator\u001b[38;5;241m.\u001b[39m_valid_schemas\u001b[38;5;241m.\u001b[39madd(_hash)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeploans/lib/python3.10/site-packages/cerberus/schema.py:283\u001b[0m, in \u001b[0;36mDefinitionSchema._validate\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m    280\u001b[0m         test_schema[field] \u001b[38;5;241m=\u001b[39m test_rules\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema_validator(test_schema, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SchemaError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema_validator\u001b[38;5;241m.\u001b[39merrors)\n",
      "\u001b[0;31mSchemaError\u001b[0m: {'AUTL16': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL20': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL24': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL25': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL26': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_int']}]}], 'AUTL29': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL30': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL31': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL33': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL37': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL38': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL39': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL40': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL43': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL44': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_int']}]}], 'AUTL45': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL46': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL47': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_int']}]}], 'AUTL48': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL49': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL50': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL51': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL52': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL59': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL6': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL60': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL61': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL62': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL63': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL64': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL65': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL66': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL67': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL68': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL69': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_int']}]}], 'AUTL7': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL72': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL73': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL74': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL75': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL76': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL77': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL78': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_number']}]}], 'AUTL8': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}], 'AUTL9': [{'coerce': ['none or more than one rule validate', {'oneof definition 0': ['must be of callable type'], 'oneof definition 1': ['must be of list type'], 'oneof definition 2': ['unallowed value to_date']}]}]}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "# 1. Setup Spark locale\n",
    "spark = SparkSession.builder.appName(\"esma_local_test\").getOrCreate()\n",
    "\n",
    "# 2. Parametri di test (modifica secondo necessità)\n",
    "LOCAL_RAW_PATH = \"./dati\"  # Dove hai i tuoi file CSV/Excel\n",
    "LOCAL_OUTPUT_PATH = \"./output\"\n",
    "DL_CODE = \"test_deal_code\"\n",
    "INGESTION_DATE = \"2025-06-01\"\n",
    "\n",
    "# 3. Importa le funzioni reali dal progetto\n",
    "from src.aut_etl_pipeline.profile_bronze_data import profile_bronze_data\n",
    "from src.aut_etl_pipeline.generate_bronze_tables import generate_bronze_tables\n",
    "from src.aut_etl_pipeline.generate_asset_silver import generate_asset_silver\n",
    "\n",
    "# 4. Adatta le funzioni per accettare path locali\n",
    "# Esempio: modifica le funzioni per accettare un flag \"local_mode\"\n",
    "# oppure, se usano direttamente GCS, copia temporaneamente il codice \n",
    "# in questo script e sostituisci GCS con l'I/O locale.\n",
    "\n",
    "# 5. Esegui la pipeline reale, lavorando su file locali\n",
    "\n",
    "# Profilazione bronze\n",
    "profile_bronze_data(\n",
    "    raw_bucketname=\"\",\n",
    "    data_bucketname=\"\",\n",
    "    source_prefix=\"./dati/Loan_Data.csv\",\n",
    "    file_key=\"Loan_Data\",\n",
    "    data_type=\"assets\",\n",
    "    ingestion_date=\"2025-06-01\",\n",
    "    local_mode=True,\n",
    "    local_output_path=\"./output\"\n",
    ")\n",
    "\n",
    "generate_bronze_tables(\n",
    "    spark=spark,\n",
    "    data_bucketname=\"\",\n",
    "    source_prefix=\"./output\",\n",
    "    target_prefix=\"./output/bronze_table\",\n",
    "    data_type=\"assets\",\n",
    "    ingestion_date=\"2025-06-01\",\n",
    "    local_mode=True\n",
    ")\n",
    "\n",
    "generate_asset_silver(\n",
    "    spark=spark,\n",
    "    bucket_name=\"\",\n",
    "    source_prefix=\"./output/bronze_table\",\n",
    "    target_prefix=\"./output/silver_table\",\n",
    "    dl_code=\"DL_CODE_TEST\",\n",
    "    ingestion_date=\"2025-06-01\",\n",
    "    local_mode=True\n",
    ")\n",
    "\n",
    "print(\"Pipeline completata! Controlla la cartella ./output per i risultati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'date_col': datetime.datetime(2025, 6, 1, 0, 0)}\n"
     ]
    }
   ],
   "source": [
    "from cerberus import Validator\n",
    "from src.aut_etl_pipeline.utils.validation_rules import asset_schema, TO_DATE, TO_NUMBER\n",
    "\n",
    "\n",
    "schema = {\n",
    "    \"date_col\": {\"type\": \"datetime\", \"coerce\": TO_DATE}\n",
    "}\n",
    "\n",
    "v = Validator(schema)\n",
    "print(v.validate({\"date_col\": \"2025-06-01\"}))  # True\n",
    "print(v.document)  # {'date_col': datetime.datetime(2025, 6, 1, 0, 0)}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
