{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-Step Guide: Testing the ESMA Auto Loans Pipeline in a Notebook\n",
    "\n",
    "This guide allows you to test the **Spark modules** of the ESMA Auto Loans pipeline directly from a notebook (Colab, Dataproc, Jupyter).  \n",
    "The goal is to simulate a pipeline run on real data by reading and writing from GCS, without going through Airflow.\n",
    "\n",
    "!If you want to use with local files go to [Simulation](#results)!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Prerequisites**\n",
    " \n",
    "- Python â‰¥ 3.8\n",
    "- Libraries: `pyspark`, `google-cloud-storage`, `pandas`, `cerberus`, `delta-spark`\n",
    "- GCP service account with permissions on GCS buckets\n",
    "- Input files uploaded to GCS (e.g., CSV files in `dl_data/downloaded-data/AUT/<dl_code>/`)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Setup Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab/Dataproc)\n",
    "!pip install pyspark==3.3.1 delta-spark==2.1.0 google-cloud-storage cerberus pandas\n",
    "\n",
    "import os\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or:**  \n",
    "Make sure your GOOGLE_APPLICATION_CREDENTIALS environment variable points to the service account json!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/to/your/service-account.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Setup SparkSession with Delta and GCS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = SparkSession.builder.appName(\"esma_test\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.1.0\") \\\n",
    "    .config(\"spark.delta.logStore.gs.impl\", \"io.delta.storage.GCSLogStore\") \\\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\") \\\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Test Parameters Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change with real values\n",
    "PROJECT_ID = \"your_project_id\"\n",
    "RAW_BUCKET = \"your_raw_bucket\"\n",
    "DATA_BUCKET = \"your_data_bucket\"\n",
    "DL_CODE = \"test_deal_code\"  # example: 'AUT1234'\n",
    "INGESTION_DATE = \"2025-05-31\"\n",
    "SOURCE_PREFIX = f\"dl_data/downloaded-data/AUT/{DL_CODE}\"\n",
    "TARGET_BRONZE_PREFIX = \"AUT/bronze/assets\"\n",
    "TARGET_SILVER_PREFIX = \"AUT/silver/assets\"\n",
    "FILE_KEY = \"Loan_Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Profilation Test Bronze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aut_etl_pipeline.profile_bronze_data import profile_bronze_data\n",
    "\n",
    "# Launch the profiling: generates the clean_dump/ and dirty_dump/ on GCS\n",
    "profile_bronze_data(\n",
    "    raw_bucketname=RAW_BUCKET,\n",
    "    data_bucketname=DATA_BUCKET,\n",
    "    source_prefix=SOURCE_PREFIX,\n",
    "    file_key=FILE_KEY,\n",
    "    data_type=\"assets\",\n",
    "    ingestion_date=INGESTION_DATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check on GCS:**  \n",
    "You should find the CSV files in `gs://<DATA_BUCKET>/clean_dump/assets/` and `dirty_dump/assets/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Test Bronze Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aut_etl_pipeline.generate_bronze_tables import generate_bronze_tables\n",
    "\n",
    "generate_bronze_tables(\n",
    "    spark=spark,\n",
    "    data_bucketname=DATA_BUCKET,\n",
    "    source_prefix=SOURCE_PREFIX,\n",
    "    target_prefix=TARGET_BRONZE_PREFIX,\n",
    "    data_type=\"assets\",\n",
    "    ingestion_date=INGESTION_DATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check on GCS:**  \n",
    "You should find the Delta tables in `gs://<DATA_BUCKET>/AUT/bronze/assets/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Test Silver Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aut_etl_pipeline.generate_asset_silver import generate_asset_silver\n",
    "\n",
    "generate_asset_silver(\n",
    "    spark=spark,\n",
    "    bucket_name=DATA_BUCKET,\n",
    "    source_prefix=TARGET_BRONZE_PREFIX,\n",
    "    target_prefix=TARGET_SILVER_PREFIX,\n",
    "    dl_code=DL_CODE,\n",
    "    ingestion_date=INGESTION_DATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check on GCS:**  \n",
    "You should find the Parquet files in `gs://<DATA_BUCKET>/AUT/silver/assets/lease_info_table/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Read and Verify Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the silver table to verify the result\n",
    "df = spark.read.parquet(f\"gs://{DATA_BUCKET}/AUT/silver/assets/lease_info_table/\")\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. (Optional) Test Deal Details**\n",
    " \n",
    "Apply the same schema, changing `data_type`, prefixes and functions (use `generate_deal_details_bronze` and `generate_deal_details_silver`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Debug**\n",
    "- If you receive errors, print the logs, check GCS permissions and the presence of input files.\n",
    "- If you have dependency errors, make sure all versions are compatible.\n",
    "- You can modify the parameters or work on a single DL_CODE for faster debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **11. Cleanup**\n",
    "\n",
    "If you want to remove test files from the buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "bucket = client.get_bucket(DATA_BUCKET)\n",
    "for blob in bucket.list_blobs(prefix=\"AUT/\"):\n",
    "    print(\"Deleting\", blob.name)\n",
    "    blob.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Simulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/03 10:33:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:33:02,214 - src.aut_etl_pipeline.profile_bronze_data - INFO - Start ASSETS BRONZE PROFILING job.\n",
      "2025-06-03 10:33:02,224 - src.aut_etl_pipeline.profile_bronze_data - INFO - Running in LOCAL mode.\n",
      "2025-06-03 10:33:02,224 - src.aut_etl_pipeline.profile_bronze_data - INFO - Processing local file: /Users/hp2/Deeploans/deeploans/ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/Auto_Loans_Synthetic_Dataset.csv\n",
      "2025-06-03 10:33:02,452 - src.aut_etl_pipeline.profile_bronze_data - INFO - Profilazione completata (LOCAL): 0 clean, 84 dirty\n",
      "2025-06-03 10:33:02,452 - src.aut_etl_pipeline.profile_bronze_data - INFO - End ASSETS BRONZE PROFILING job.\n",
      "2025-06-03 10:33:02,453 - src.aut_etl_pipeline.generate_bronze_tables - INFO - Start ASSETS BRONZE job.\n",
      "2025-06-03 10:33:02,453 - src.aut_etl_pipeline.generate_bronze_tables - INFO - Running in LOCAL mode.\n",
      "2025-06-03 10:33:04,541 - src.aut_etl_pipeline.generate_bronze_tables - INFO - Bronze table (LOCAL) scritta in ./output/bronze_table\n",
      "2025-06-03 10:33:04,542 - src.aut_etl_pipeline.generate_asset_silver - INFO - Start ASSET SILVER job.\n",
      "2025-06-03 10:33:04,604 - src.aut_etl_pipeline.generate_asset_silver - INFO - Apply enrichment for LOCAL mode\n",
      "2025-06-03 10:33:04,798 - src.aut_etl_pipeline.generate_asset_silver - INFO - Silver table (LOCAL) scritta in ./output/silver_table\n",
      "Pipeline completata! Controlla la cartella ./output per i risultati.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "# 1. Setup Spark local\n",
    "spark = SparkSession.builder.appName(\"esma_local_test\").getOrCreate()\n",
    "\n",
    "# 2. Test parameters (modify as needed)\n",
    "LOCAL_RAW_PATH = \"ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/Auto_Loans_Synthetic_Dataset.csv\"  # Where you have your CSV/Excel files\n",
    "LOCAL_OUTPUT_PATH = \"./output\"\n",
    "DL_CODE = \"test_deal_code\"\n",
    "INGESTION_DATE = \"2025-06-01\"\n",
    "\n",
    "# 3. Import the real functions from the project\n",
    "from src.aut_etl_pipeline.profile_bronze_data import profile_bronze_data\n",
    "from src.aut_etl_pipeline.generate_bronze_tables import generate_bronze_tables\n",
    "from src.aut_etl_pipeline.generate_asset_silver import generate_asset_silver\n",
    "\n",
    "# 4. Adapt functions to accept local paths\n",
    "# Example: modify functions to accept a \"local_mode\" flag\n",
    "# or, if they use GCS directly, temporarily copy the code \n",
    "# in this script and replace GCS with local I/O.\n",
    "\n",
    "# 5. Execute the real pipeline, working on local files\n",
    "\n",
    "# Profilazione bronze\n",
    "profile_bronze_data(\n",
    "    raw_bucketname=\"\",\n",
    "    data_bucketname=\"\",\n",
    "    source_prefix=LOCAL_RAW_PATH,\n",
    "    file_key=\"Loan_Data\",\n",
    "    data_type=\"assets\",\n",
    "    ingestion_date=\"2025-06-01\",\n",
    "    local_mode=True,\n",
    "    local_output_path=\"./output\"\n",
    ")\n",
    "# Generate the bronze table\n",
    "generate_bronze_tables(\n",
    "    spark=spark,\n",
    "    data_bucketname=\"\",\n",
    "    source_prefix=\"./output\",\n",
    "    target_prefix=\"./output/bronze_table\",\n",
    "    data_type=\"assets\",\n",
    "    ingestion_date=\"2025-06-01\",\n",
    "    local_mode=True\n",
    ")\n",
    "# Generate the silver table\n",
    "generate_asset_silver(\n",
    "    spark=spark,\n",
    "    bucket_name=\"\",\n",
    "    source_prefix=\"./output/bronze_table\",\n",
    "    target_prefix=\"./output/silver_table\",\n",
    "    dl_code=\"DL_CODE_TEST\",\n",
    "    ingestion_date=\"2025-06-01\",\n",
    "    local_mode=True\n",
    ")\n",
    "\n",
    "print(\"Pipline completed! Check the ./output folder for the results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime righe dati dirty:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIELD CODE</th>\n",
       "      <th>FIELD NAME</th>\n",
       "      <th>CONTENT TO REPORT</th>\n",
       "      <th>LOAN 1</th>\n",
       "      <th>LOAN 2</th>\n",
       "      <th>LOAN 3</th>\n",
       "      <th>LOAN 4</th>\n",
       "      <th>LOAN 5</th>\n",
       "      <th>__errors__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUTL1</td>\n",
       "      <td>Unique Identifier</td>\n",
       "      <td>The unique identifier assigned by the reportin...</td>\n",
       "      <td>AUT-2024-001-A</td>\n",
       "      <td>AUT-2024-002-B</td>\n",
       "      <td>AUT-2024-003-C</td>\n",
       "      <td>AUT-2024-004-D</td>\n",
       "      <td>AUT-2024-005-E</td>\n",
       "      <td>{'CONTENT TO REPORT': ['unknown field'], 'FIEL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUTL2</td>\n",
       "      <td>Original Underlying Exposure Identifier</td>\n",
       "      <td>Unique underlying exposure identifier</td>\n",
       "      <td>ORIG-EXP-001</td>\n",
       "      <td>ORIG-EXP-002</td>\n",
       "      <td>ORIG-EXP-003</td>\n",
       "      <td>ORIG-EXP-004</td>\n",
       "      <td>ORIG-EXP-005</td>\n",
       "      <td>{'CONTENT TO REPORT': ['unknown field'], 'FIEL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTL3</td>\n",
       "      <td>New Underlying Exposure Identifier</td>\n",
       "      <td>New identifier if changed from AUTL2</td>\n",
       "      <td>ORIG-EXP-001</td>\n",
       "      <td>ORIG-EXP-002</td>\n",
       "      <td>ORIG-EXP-003</td>\n",
       "      <td>ORIG-EXP-004</td>\n",
       "      <td>ORIG-EXP-005</td>\n",
       "      <td>{'CONTENT TO REPORT': ['unknown field'], 'FIEL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUTL4</td>\n",
       "      <td>Original Obligor Identifier</td>\n",
       "      <td>Original unique obligor identifier</td>\n",
       "      <td>OBLG-2024-001</td>\n",
       "      <td>OBLG-2024-002</td>\n",
       "      <td>OBLG-2024-003</td>\n",
       "      <td>OBLG-2024-004</td>\n",
       "      <td>OBLG-2024-005</td>\n",
       "      <td>{'CONTENT TO REPORT': ['unknown field'], 'FIEL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUTL5</td>\n",
       "      <td>New Obligor Identifier</td>\n",
       "      <td>New identifier if changed from AUTL4</td>\n",
       "      <td>OBLG-2024-001</td>\n",
       "      <td>OBLG-2024-002</td>\n",
       "      <td>OBLG-2024-003</td>\n",
       "      <td>OBLG-2024-004</td>\n",
       "      <td>OBLG-2024-005</td>\n",
       "      <td>{'CONTENT TO REPORT': ['unknown field'], 'FIEL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FIELD CODE                               FIELD NAME  \\\n",
       "0      AUTL1                        Unique Identifier   \n",
       "1      AUTL2  Original Underlying Exposure Identifier   \n",
       "2      AUTL3       New Underlying Exposure Identifier   \n",
       "3      AUTL4              Original Obligor Identifier   \n",
       "4      AUTL5                   New Obligor Identifier   \n",
       "\n",
       "                                   CONTENT TO REPORT          LOAN 1  \\\n",
       "0  The unique identifier assigned by the reportin...  AUT-2024-001-A   \n",
       "1              Unique underlying exposure identifier    ORIG-EXP-001   \n",
       "2               New identifier if changed from AUTL2    ORIG-EXP-001   \n",
       "3                 Original unique obligor identifier   OBLG-2024-001   \n",
       "4               New identifier if changed from AUTL4   OBLG-2024-001   \n",
       "\n",
       "           LOAN 2          LOAN 3          LOAN 4          LOAN 5  \\\n",
       "0  AUT-2024-002-B  AUT-2024-003-C  AUT-2024-004-D  AUT-2024-005-E   \n",
       "1    ORIG-EXP-002    ORIG-EXP-003    ORIG-EXP-004    ORIG-EXP-005   \n",
       "2    ORIG-EXP-002    ORIG-EXP-003    ORIG-EXP-004    ORIG-EXP-005   \n",
       "3   OBLG-2024-002   OBLG-2024-003   OBLG-2024-004   OBLG-2024-005   \n",
       "4   OBLG-2024-002   OBLG-2024-003   OBLG-2024-004   OBLG-2024-005   \n",
       "\n",
       "                                          __errors__  \n",
       "0  {'CONTENT TO REPORT': ['unknown field'], 'FIEL...  \n",
       "1  {'CONTENT TO REPORT': ['unknown field'], 'FIEL...  \n",
       "2  {'CONTENT TO REPORT': ['unknown field'], 'FIEL...  \n",
       "3  {'CONTENT TO REPORT': ['unknown field'], 'FIEL...  \n",
       "4  {'CONTENT TO REPORT': ['unknown field'], 'FIEL...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the result \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_dirty = pd.read_csv(\"ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/output/dirty_dump/assets/dirty_Loan_Data_2025-06-01.csv\")\n",
    "\n",
    "print(\"First rows of dirty data:\")\n",
    "\n",
    "df_dirty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/output/bronze_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m bronze_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/output/bronze_table\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m silver_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/output/silver_table\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df_bronze \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbronze_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df_silver \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(silver_path)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBronze table:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeploans/lib/python3.10/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeploans/lib/python3.10/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeploans/lib/python3.10/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeploans/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/output/bronze_table'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bronze_path = \"ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/output/bronze_table\" # Insert the path to the bronze table  \n",
    "silver_path = \"ETL-Pipelines/ESMA-Loan-level-data-templates/Auto_Loans_ESMA/output/silver_table\" # Insert the path to the silver table\n",
    "df_bronze = pd.read_parquet(bronze_path)\n",
    "df_silver = pd.read_parquet(silver_path)\n",
    "\n",
    "print(\"Bronze table:\")\n",
    "print(df_bronze.head())\n",
    "\n",
    "print(\"Silver table:\")\n",
    "print(df_silver.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
